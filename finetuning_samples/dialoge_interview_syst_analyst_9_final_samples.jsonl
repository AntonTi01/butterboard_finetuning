{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Так, да, привет ещё раз. Расскажи в целом… Ну, так, объясню, наверное, у нас сейчас будет собес. Вот мы пробежимся по темам некоторым, которые участвуют у нас в грейдах, в точке в целом. Поговорим про инфраструктуру и так далее. Вот. Расскажи в целом про команду, про проекты. Какую роль ты занимал в команде? Может, что-то интересное вспомнишь? А, ещё раз всем привет! Да, в основном у меня опыт был в интехе.\nВопрос: Три-четыре года был WebMoney опыт. Касаемо проектов, самые интересные это были C2C, то есть у нас была внутренняя валюта в рамках компании, и её нужно было переводить, там, к примеру, в доллары, в рубли, то есть некоторая биржевая, так сказать, Некоторые биржи. Вот. Этот проект у нас был на Monolith. В основном из стека у нас был ASP.NET Core. Последняя версия — это был на тот момент.NET 6-й. Потом мы его тоже переписывали уже на микросервис и переносили. Был опыт с EF-Core, с Dapper. По базам данных был Postgres, SQL-сервер, Docker, то есть все вот эти новомодные штуки. Что касается самой команды, у нас она состояла из 10 до 15 человек примерно в этом интервале. Были в основном по большей части бэкэнд-разработчики на.NET и Java. Был один QI, был архитектор и был Team Lead. Вот. Второй проект — это был СБП, то есть система быстрых платежей. Нужно было как раз, чтобы в нашей системе были переводы по системе СБП. Вот. Этот проект уже был на микросервисах, сразу написан. Поэтому тоже был опыт с монолитом и с микросервисами.\nОтвет: Вот, как ты… Да какую роль в целом в команде занимал? То есть какие-то задачи, может, сам грумил?\nВопрос: У нас обучение было, да. Сперва я пришёл вообще в компанию стажёром, вот, потом вырос до сеньора. Тоже был менторинг, там два-три человека в рамках своей команды обучал. В основном просто разработчик, но вот когда мы переписывали условно на микросервисы, У меня с ними был, так сказать, небольшой опыт, но он был. Вот, там тоже нужно было принимать некоторые решения.", "output": {"hard_skills": ["ASP.NET Core", ".NET 6", "EF-Core", "Dapper", "PostgreSQL", "SQL Server", "Docker", "Монолитная архитектура", "Микросервисы", "Менторинг", "Переход с монолита на микросервисы"], "soft_skills": ["Менторство", "Работа в команде", "Принятие решений", "Коммуникабельность", "Адаптивность"], "recommendations": ["Углубить знания в области микросервисной архитектуры и облачных технологий.", "Развивать лидерские навыки для эффективного управления командой.", "Улучшить навыки коммуникации для более эффективного взаимодействия с различными стейкхолдерами.", "Участвовать в проектах, требующих масштабируемых решений, для расширения опыта."]}, "chunk_id": 0, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Хорошо. Я думаю, сразу к теме будем переходить, к первой. Давай про базы поговорим. Расскажи, с какими базами работал, опыт, кейсы, может быть.\nВопрос: Угу. В основном были базы реляционные — это SKL сервер. Потом мы делали миграцию на Postgres. Вот. По новый SKL-им базам данных у нас был Mongo, Clickhouse, если его тоже можно, по сути, назвать. Но там взаимодействия было с ними мало, в основном реляционки. Касаемо Каких кейсов? Ну, это оптимизация у меня была с индексами работа, с джойнами, хронимки переписывал на EF Core, потом с ними тоже у меня был опыт. А что ещё?\nОтвет: Ну вот, прям, полноценный базовый… Сейчас уже, вот, ты, ну, с Postgres-ом, правильно, работаешь? Да, сейчас с Postgres-ом. А ты, вот, насколько глубоко, там, копал, как он устроен, про внутрянку, там, уровни изоляции, вот это вот всё?\nВопрос: Ну, про уровни изоляции и транзакций разбирал это. Про внутрянку даже был небольшой опыт, прям, с настройкой PG-конфа, вот, но не сказать, чтобы прям как-то детально. Но внутрянки, внутренние моменты базы данных копал прям хорошо разбирался, потому что на проекте у нас был прям очень сильный SQL-разработчик и получал от него опыт. А вот в целом, вот в Postgre, какие уровни изоляции, например, есть? Самый минимальный. Так, а мне слышно, да? Да, да. Ага. А, ну, по сути, в рамках нескольких баз данных, вот SQL Server у нас четыре, а в рамках Postgres у нас их тоже четыре, но уровень read and Commit там его нет. По умолчанию у нас Read and Commit идет.", "output": {"hard_skills": ["SQL Server", "PostgreSQL", "MongoDB", "ClickHouse", "EF Core", "миграция баз данных", "оптимизация индексов", "работа с джойнами", "уровни изоляции транзакций", "настройка PostgreSQL конфигурации"], "soft_skills": ["умение работать в команде", "навыки сотрудничества", "способность учиться у коллег", "адаптивность", "решение проблем"], "recommendations": ["Углубить знания во внутренней архитектуре PostgreSQL для более эффективной оптимизации и настройки базы данных.", "Продолжить развивать навыки работы с нереляционными базами данных, такими как MongoDB и ClickHouse, для расширения технического арсенала.", "Развивать навыки документирования технических решений и процессов оптимизации.", "Улучшать навыки коммуникации для эффективного взаимодействия со всеми участниками проекта."]}, "chunk_id": 1, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Окей, а ты вот сам как-то пользовался?\nВопрос: Ну вот, уровнями изоляции, может, какую-то транзакционность разрулила, такую сложную, или не было кейсов? А был опыт, у нас в основном был, опять, это же финтех, очень много финансовых операций, и вот когда были какие-то сложные транзакции по переводу денежных средств, там я использовал уровень «серилайзабл», чтобы была максимальная согласованность данных. Так же было использование уровня Read Committed, ну, там Для Таски уже, честно, не вспомню, но это вроде бы связано с отчётами. Там не нужна была дикая какая-то согласованность данных, нужна была максимальная производительность. Окей.\nОтвет: А если бы индексы затронуты? То есть, ты, получается, сам проектировал таблички, да, самостоятельно? Вот расскажи, чем руководствуешься, когда размечаешь индексы?\nВопрос: А, ну, сперва как, да, сперва, понятно, нужно спросить у, в целом, команды, там, кто пользуется системой, по каким столбцам, допустим, будет очень активный поиск идти. Это, там, сам либо я как-то понимаю, либо спрашиваю, условно, вот, и поактивно, ну. По каким столбцам у нас будет идти очень активный поиск? Соответственно, самое такое первое решение, которое приходит в голову, это навесить индексы. А второе, когда уже условно таблица сформирована, Обычно приходит, условно, задачка, там, что запросы выполняются долго, надо посмотреть, вот. И ты идешь, там, смотреть либо план выполнения запросов, и параллельно раскручиваешь систему, есть ли индексы. Если есть, то почему они, условно, не выполняют свою роль, условно, по ускорению запросов? Вот, то есть это в моменте уже разбираешься.", "output": {"hard_skills": ["уровни изоляции транзакций", "транзакционность", "работа с финансовыми операциями", "использование уровней изоляции 'SERIALIZABLE' и 'READ COMMITTED'", "проектирование и оптимизация индексов", "анализ планов выполнения запросов", "оптимизация производительности баз данных"], "soft_skills": ["умение работать в команде", "коммуникабельность", "умение выявлять и решать проблемы", "аналитическое мышление", "направленность на производительность и эффективность"], "recommendations": ["Углубить знания и практические навыки работы с различными уровнями изоляции транзакций для лучшего понимания их влияния на систему.", "Развивать навыки проектирования баз данных, уделяя особое внимание выбору и оптимизации индексов для повышения производительности.", "Освоить более продвинутые техники анализа планов выполнения запросов для эффективной оптимизации сложных запросов.", "Развивать навыки взаимодействия со стейкхолдерами для более эффективного сбора требований и согласования решений.", "Практиковаться в решении реальных кейсов, связанных с высокой нагрузкой и отказоустойчивостью, чтобы укрепить опыт в этих областях."]}, "chunk_id": 2, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: О, а ты тут про план запросов задел. А знаешь, чем отличается вот Explain от Explain Analyze в\nВопрос: «Explain, explain, analyze» — да, я использовал. Там вроде бы, когда мы используем «explain», мы в принципе собираем статистику, ну то есть как бы чтобы нам статистика отобразилась. А с «explain, analyze» мы вроде бы её как раз прогоняем, то есть делаем загрузку всех данных, и, соответственно, у нас как бы При первом запуске запрос выполняется дольше с explain-анализом, а второй раз уже будет быстрее, потому что вся необходимая инфа загрузилась. Ага, всё так. А вот если к индексам вернуться, получается\nОтвет: А какие вообще минусы у индексов?\nВопрос: Почему на все колонки, например, их не махнуть? Да, самая главная проблема при использовании индексов — это их размер, то есть они занимают некоторую память, вот. Если мы, в принципе, на все столбцы навешаем индексы, это может привести к тому, что у нас очень быстро разрастаться будет по памяти наша таблица. Вот. И второй Вторая проблема — это когда у нас очень часто есть запросы на insert, update, delete, то при Вот в этих операциях нам нужно, собственно, перестраивать индекс. Нужно, чтобы полностью его перестроить, и чтобы он построился на новых данных. Ну и третье, еще могу сказать, это, условно, когда у нас табличка маленькая, индексы не всегда могут вообще играть роль. Условно, у нас... База данных как бы сама понимает, когда использовать план веб… Ну, использовать индекс или нет. Если, условно, таблица маленькая, мы можем просто сиквью-сканом пробежаться по ней, и этого будет достаточно. Угу, прикольно. А давай такую задачку попробуем решить, вот, например.", "raw_response": "{\n  \"output\": {\n    \"hard_skills\": [\n      \"SQL\",\n      \"Использование EXPLAIN и EXPLAIN ANALYZE\",\n      \"Управление индексами\",\n      \"Оптимизация запросов\",\n      \"Пони", "chunk_id": 3, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: У тебя есть приложение, в которое большой трафик, много запросов летит на вставку и на чтение. Вот у тебя табличка из какой-то колонки, да, на вставку в эту колонку и на чтение этой колонки. У тебя задача, вот, поменять в этой колонке тип например там со строки на число ну допустим на или наоборот вот расскажи поэтапно как бы ты Ты это делал, чтобы, там, пользователи не пострадали, чтобы сохранить вот этот вот… все данные, которые к нам приходят, и чтобы чтение не нарушать?\nВопрос: Угу. Ну, если, условно, у нас в моменте это надо сделать, то я бы к этой таблице ничего не применял бы, никаких изменений, потому что вдруг, если что-то там, условно, ну, сбой какой-то произойдёт, то могут пострадать данные. А я бы просто создал, условно, аналог этой же таблицы. Добавил бы как раз… Заменил бы вот это поле на новый тип и параллельно, условно, какими-то партициями, пачками вносил бы новые туда изменения. А давай вот поконкретнее. Вот, например, у нас вот первый релиз. Ну, я бы по-любому начал бы создавать просто новую таблицу. И, так сказать, с новым типом. Тут, наверное, можно колонку просто создать. Ну вот, а, ну можно новую колонку, кстати, да. И пока, если, условно, такая нагрузка, именно в ту колонку добавлять новые данные. Но надо только понять, как их потом джойнить потом. Вот в этом проблема. Можно, кстати, если… Ну, по-любому, нам нужно будет как-то их там между собой связывать и понимать, какой они записи конкретно делают, добавляются. Можно было бы добавить какой-то внешний ключ. Чтоб потом с помощью join-а их получить и добавить, условно, в новую следующую таблицу, которую мы будем также создавать. Ну, просто колонки в следующий раз добавим больше новых из первой таблицы.\nОтвет: Ну то есть, получается, с чтением мы ничего, да, не делаем, получается, читаем из старой тоже.\nВопрос: В первом релизе. И старый. Ну, то есть.", "output": {"hard_skills": ["проектирование схем баз данных", "миграция данных", "управление высоконагруженными приложениями", "знание SLA", "обеспечение отказоустойчивости"], "soft_skills": ["решение проблем", "планирование", "внимательность к деталям", "коммуникабельность", "умение работать с командой и стейкхолдерами"], "recommendations": ["Углубить знания в области миграции данных и работы с высоконагруженными системами.", "Развивать навыки планирования и управления проектами для более эффективного выполнения сложных задач.", "Улучшить навыки документирования процессов миграции для обеспечения прозрачности и повторяемости.", "Работать над улучшением коммуникационных навыков для более эффективного взаимодействия с командой и стейкхолдерами.", "Изучить дополнительные методы обеспечения отказоустойчивости и гарантирования непрерывности работы приложения."]}, "chunk_id": 4, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Блин, я бы всё-таки их перенёс бы как-то. Вот если они прям мегаогромные, ты в моменте их не можешь перенести, то есть это простое какое-то будет. То есть ты нарушишь чтение, если ты начнёшь их переносить.\nВопрос: Ну, я бы тогда добавлял бы в старую, ну, сделал бы чтение из старой таблицы, к примеру, а потом уже, когда у нас всё наладилось, мы уже могли поэтапно, ну, вот как я говорил, пачками старые данные переводить в новые, условно так. А, то есть несколько релизов понадобится. Да, да, да, ну потому что разом, да, большой скоп данных, как бы, я не видел за историей, что вот такое используется. Мы вот именно как-то делили партийцами, условно, и их так внедряли.\nОтвет: У меня, Борис, есть что докинуть?\nВопрос: Да, с JSON-ом я работал. У меня был опыт прикручивания OAuth 2 протокола в нашу систему. И там нужно было… Ну, допустим, Не знаю, «Одноклассники», да, возьмем в пример, конечно. Нужно было, условно, сделать авторизацию с помощью «Одноклассников», и нужно было у конкретного пользователя, кто делает через этот сервис авторизацию, подтянуть его друзей. Собственно, эта опишка возвращала некоторый JSON-формат. И вот эти настройки нужно было как раз у нас хранить именно в JSON. вот в виде строки Это нужно было Ну, хранить их потом при получении, условно, происходит там диссерилизация, всё очень быстро маппиться к нужным свойствам и, собственно, могли дальше как-то эти данные обрабатывать. Вот. А когда нужно использовать таблички, Пример. За место Джейсона, да, именно, в столбца. Тут, наверное, вопрос, вот почему-то больше меня склоняет именно к нормализации и денормализации. А вот, к примеру, если используем термин «денормализация», иногда может быть такая ситуация, что нам скоупом, вот именно в рамках одного запроса, нужно получить опцию. в рамках одного запроса именно JSON-ом. Мы его получаем, и его можем на практике как-то использовать. Если мы говорим про обычную таблицу, то это уже нормализация как бы идет, А одним скопом запроса мы как бы не можем подтянуть данные. Там, по-любому, у нас будут джойны идти. Скорее всего, речь про это, я так предполагаю. С какими локами? Вот что-то из головы прям вылетело. Ну, я слышал про хинты на практике использовал, там, with no lock. А, скорее всего, локи про прям лок на таблицу и лок конкретно на...", "output": {"hard_skills": ["Описание функциональных и нефункциональных требований", "SLA (Service Level Agreement)", "Нагрузочные параметры", "Требования к отказоустойчивости", "Работа с JSON", "Прикручивание OAuth 2 протокола", "Авторизация через социальные сети (например, Одноклассники)", "Сериализация и десериализация данных", "Нормализация и денормализация баз данных", "Использование хинтов в SQL (например, WITH NOLOCK)"], "soft_skills": ["Коммуникабельность", "Умение согласовывать требования с командой разработки и стейкхолдерами", "Проблемное мышление", "Способность делить задачи на этапы и контролировать имплементацию"], "recommendations": ["Углубить знания в области работы с базами данных, особенно в нормализации и денормализации, чтобы более эффективно проектировать структуры данных.", "Развивать навыки четкой и структурированной коммуникации для улучшения взаимодействия с командой и стейкхолдерами.", "Изучить дополнительные методы оптимизации запросов и управления транзакциями для повышения производительности систем.", "Практиковаться в документировании решений и процессов, чтобы сделать их более понятными для всех участников проекта."]}, "chunk_id": 5, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: А строки приходилось лачить?\nВопрос: Вроде бы за практику было, я вот такое вспоминаю. Селект фор апдейт. Если это речь про это, это же, по сути, тоже… Селект фор апдейт, когда мы выполняем, это вроде бы лог на конкретную строку. То есть в рамках отдельного, допустим… Потока мы можем… Ну, вот это тоже с брокерами в сообщении встречается. Как раз я про outbox-паттерн его использовал. Когда select for update дело для записи, мы в рамках как бы одного потока лодчим, вторая, собственно, ничего сделать не может, пока мы не выполним операцию в первой, ну, транзакции, так назовем.\nОтвет: Спасибо. Ладно, давайте погоним дальше, как раз к вот КРМ перейдём.\nВопрос: Ты тоже расскажи про УРМ, там, с какими работал, какие нравятся, может, какие-то интересные истории. В основном был опыт с эвкором. По большей части где-то два с половиной года только на нём работал. Что касаемо интересных практик, ну, использовал. За загрузки данных, не знаю, можно ли сказать, что это интересно было. Я, так сказать, разбирался с лишними инклюдами, которые были. Был опыт написания inner join-а, вот тоже я помню task-у, потому что include — это обычно left join. Потом… А, конфигурационные файлы. У нас всё было в рамках метода onModelCreating. Я вносил практику Так сказать, распределение по конфигурационным файлам, чтобы всё было очень удобно. Вот, что ещё? Про чиндж-трекер, ну, тут не скажу, что сильно сталкивался, просто про него слышал, там, S-Mode трекинг по мере необходимости использовал. Даппер был, даппер, вот, но с даппером, честно, ну, больше нравится F-Core, не знаю, как будто больше нравится именно с ним работать, копать, как он под капотом устроен, вот, но с даппером тоже был опыт. Небольшой, но был. А вот ты же ты говоришь, «Include», говоришь, он «inner join». А вот он всегда «inner join»? Не-не-не, я «I», я сказал. Нет, «include» — он «left join». Когда нам нужно сделать «inner join»? Вот, кстати, если... Ну, мы можем сами написать, к примеру, select main и group by, вроде бы комбинация, и тогда у нас получится inner join. Либо использовать методы операторов, конкретно этот, SQL-запрос, там, throw, inner написать, если я не ошибаюсь. Надо почитать. Я просто слышал, что он только left-joined всегда, и поэтому мы select many пишем. Вот, ну, почитаю про это. Можешь рассказать поподробнее, что за загрузка дана? Да, у нас в рамках Эвкора, чтобы получить, условно, навигационные свойства, есть три подхода. Это ленивая загрузка, когда мы просто пишем этот include, и он прямо в рамках одного запроса подгружает зависимые данные. Бывает полезно, когда нам сразу нужны данные, но когда мы их не используем, навигационные, это может только нам ухудшить по производительности, потому что время тратится. На join. Вот. Это ленивое. Явное — с помощью обращения к контексту и вызова метод entry. Ну, вызываем метод «Entry», прокидываем сущность и в зависимости от дальшей цепочки методов подгружаем либо один объект, либо коллекцию объектов. Вот. Что у нас? «Ленивая»? «Ленивая» вроде бы с помощью прокси. Там библиотека отдельно ставится, как я помню. Прокси, как там, «Entity Framework Core», и там она построена на виртуальных свойств. Вся Роль в том, что мы, к примеру, когда подгружаем основную сущность, и потом обращаемся к зависимой, она у нас только при обращении подгружается.", "output": {"hard_skills": ["SQL: SELECT FOR UPDATE", "Outbox-паттерн", "Entity Framework Core", "Dapper", "Lazy Loading", "Explicit Loading", "Change Tracking", "Настройка ORM", "Конфигурационные файлы"], "soft_skills": ["Проблемное мышление", "Умение работать с паттернами", "Адаптивность", "Коммуникабельность"], "recommendations": ["Углубить знания в области Entity Framework Core, особенно в аспектах загрузки данных и оптимизации запросов.", "Развивать навыки ясной и структурированной коммуникации для более эффективного донесения своих мыслей во время интервью.", "Изучить более детально работу с Dapper и сравнить его возможности с Entity Framework Core для расширения инструментов разработки.", "Практиковаться в использовании различных паттернов проектирования для повышения гибкости и эффективности разработки."]}, "chunk_id": 6, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: А, ну я в целом хотел про Change Tracker узнать. Так, сколько? А ты в целом, вот, ну, понимаешь примерно, что он там делает под капотом? На что он вообще повли… влиять может?\nВопрос: Да, я прям в рамках компании делал, ну, курс по EF Core и разбирал эту тему. Там прикол в том, что когда мы, допустим, получаем данные с бдшки, вот с помощью EF Core, у нас как бы под капотом создается слепок изменений. И если мы в моменте что-то редактируем и вызываем этот self-change, у нас проверяются изменения, А вот конкретно какая запись изменилась, она Выставляется ей определенный статус, там, «Modified», и она будет изменяться. Если вообще никаких изменений не было, то у нас вроде бы статус выставляется «Unchanged», то есть никак не изменено, и… Соответственно, сохранений никаких не будет происходить. Статусов может быть несколько — «Modified», «Edit», «Deleted», вроде бы, или «Remove» называется. Вот. А этот Change Tracker, он не всегда бывает, так сказать, нужен при, допустим, Get-операциях, когда нам ничего не надо изменять. Мы можем его отключить, вызвать метод «As Now Tracking», и Change Tracker будет у нас, ну, disabled, условно, выключен. Как-то так.\nОтвет: но в объекте, как вы настраивали. А может, диагонально его настраивали, вот, относительно запроса?\nВопрос: Нет, у нас он просто был по мере необходимости запросов. Вот, то есть, в моменте я что-то пишу там, и вот вызывал S-Node Tracking. Да, понятно, что, ну, это, наверное, нехорошая практика. Скорее всего, проще написать какую-то кастомную для Get-операций, которой не надо ничего менять, и вызывать его. Но вот у нас в моменте запроса выписали его, вызывали.", "raw_response": "{\n  \"output\": {\n    \"hard_skills\": [\"Entity Framework Core\", \"Change Tracker\", \"Управление состояниями объектов\", \"SLA\", \"Нагрузочные параметры\", \"Отказоустойчивость\"],\n    \"soft_skills\": [\"Коммуникабельность\", \"Умение работать в команде\", \"Проблемное мышление", "chunk_id": 7, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: А там вот глобальные у него настройки есть, и там вот есть «No tracking with identity resolution». Вот туда не копал, не смотрел, что это такое. Вот, кстати, разбирал, да. Там два метода у нас есть, но разницу я уже сейчас не назову, потому что когда я разбирался, у меня голова кипела, сейчас точно не вспомню. ну там он в общем трекает только одесшники сущности чтобы не было дублей там при дикартова перемножение вот это вот все такая настройка баланс а в целом а спид куэри вот такие штуки юзал приходилось\nВопрос: А сплитквери — это вроде бы разбиение… Или нет, сплит-квери, сейчас я вспомню. У меня было на практике, когда нужно было получить, допустим, N записей из первой таблицы, поставить между ними тире и подгрузить данные из второй таблицы. Вот это, наверное, про сплит-квери. Что-то такое вспоминаю.\nОтвет: Ну да, он там все джойны выворачивает в отдельные селекты, то есть Это полезно, когда у тебя Декартова перемножение Из-за джойнов происходит, вот, взрыв данных. Распухает вот это решение, как вариант. Через отдельные. То есть он три запроса просто делает. Ну, примерно, да. А миграции как у вас?\nВопрос: Ты пользовались EF Core, а миграции как были устроены? Тоже через EF? Да, миграции были. Ну, у нас от проекта к проекту были свои приколы, а на проекте, которым я работал, то есть, у нас было всё через миграции. Там, апдейт базы, миграция, удаление, там, таблицы какой-то или колонки — тоже была миграция, собственно. А, ну, ещё у нас был прикол через отдельные скейл-файлы. То есть, у нас и была папочка «Migrations», там", "output": {"hard_skills": ["Entity Framework Core", "Миграции в EF Core", "Split Queries", "Джойны", "Отслеживание сущностей", "Identity Resolution"], "soft_skills": ["Коммуникабельность", "Способность к решению проблем", "Адаптивность"], "recommendations": ["Углубить понимание механизмов работы Entity Framework Core, особенно в области отслеживания изменений и разрешения идентификаторов.", "Практиковать четкое и уверенное объяснение технических концепций, чтобы улучшить коммуникацию с командой и интервьюерами.", "Развивать навыки запоминания и воспроизведения сложных технических деталей для повышения уверенности во время интервью.", "Изучить дополнительные методы оптимизации запросов и управления миграциями для повышения эффективности разработки."]}, "chunk_id": 8, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: А вот как вы их менеджили? То есть, как деплоили, например, приложение?\nВопрос: Вот этот флоу был как устроен? Преинграцию, вы хотите? Да, честно, я прям глубоко не вкопал в них, но видел, что у нас были несколько схем базовых данных, и в зависимости от той схемы, которую мы используем, там, допустим, у нас была паблик, условно, базовая, и какая-то для тестов своя, с определёнными колонками. Ну, накатывали, условно указывали схему, и к этой схеме БДшки делали просто... Релиз. Вот, как конкретно внутренне сильно не копал. Мне просто последний опыт с миграциями был небольшой, поэтому вот что успел посмотреть, то посмотрел. Ага, то есть мне вот просто понять, вот при релизе именно вот миграция в какой момент накатывалась у вас? То есть, ну, пайплайны — у нас такого не было, это как отдельно для миграций. По сути, был просто CICD, ну, там опыта с базами не было вообще никакого, чисто CICD TeamCity-ов. Угу, то есть, миграция обратно совместимая, да, её можно заранее, там, выкатить и потом катить приложение, условно, вот, плюс-минус, так, да? Да, но я не знаю, какие могут быть в этом проблемы, потому что, опять, использовал, ну, небольшой опыт с ними был, вот, и вот чисто вот такая практика была.\nОтвет: А ещё такой вопрос про внутренний пул подключений к базе. Вот, у РЭМ, например, RedBug SQL представляет свой внутренний пул, и вот Как он у вас был настроен на проекте? Знаешь ли ты, как его, там, поднастроить можно? Сталкивались, может, с проблемой выжирания коннектов к базе, например, под большой нагрузкой, или что-то в такие моменты?\nВопрос: Я видел, что у нас вперва использовался на проекте просто AddDbContext, а потом AddDbContectionPool, то есть, да, пул контекстов. С саму настройку я прямо в рамках работы не сталкивался, то есть у меня не было таких задач, но, как я помню, там были свойства Max Size Pool, вроде бы максимальное количество полов одновременных на коннекшены, и сам я с ним игрался чисто вот на практике, ну, в своём подпроекте смотрел работу внутреннюю. За какие-то более внутренние тонкости Блин, ну, честно, вот я помню, Max Size Pool был свойством, максимальное количество подключений.", "output": {"hard_skills": ["Функциональное описание требований", "Нефункциональное описание требований", "SLA", "Нагрузочные параметры", "Отказоустойчивость", "Управление миграциями баз данных", "CI/CD с использованием TeamCity", "AddDbContext и AddDbConnectionPool", "Настройка пула подключений к базе данных"], "soft_skills": ["Умение работать в команде", "Коммуникабельность", "Способность согласовывать требования с стейкхолдерами", "Самоосознание ограниченного опыта и готовность учиться"], "recommendations": ["Углубить знания в области миграций баз данных и освоить современные инструменты для их управления.", "Развивать навыки настройки и оптимизации CI/CD пайплайнов, возможно, изучить другие системы помимо TeamCity.", "Получить практический опыт работы с пулами подключений и настройкой баз данных под высокие нагрузки.", "Продолжать совершенствовать навыки коммуникации и согласования требований с различными командами и стейкхолдерами.", "Изучить передовые практики в области отказоустойчивости и масштабируемости систем."]}, "chunk_id": 9, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Тот в connection строке, да, ты имеешь в виду? Просто db.context.pool же ещё у EF-а есть просто сущности, и db.context.pool их пулит, чтобы не генерить много. Ну да, я, скорее всего, про… Ну вот, когда мы подключаем adb.context, у нас там использовался метод adb.context.pool.\nВопрос: Ну, по сути, просто вытаскивание её из Upsetting.json-а и прокидывание в метод, там, usePG… useNPGSQL connection, там, вроде бы так назывался.\nОтвет: И вот в этой строке настраивается ещё внутренний пул подключения самих. Ты вот… Вы его там не конфигурили, да? То есть он у вас включен, выключен был. Честно, не скажу. Не помню этого. Ну окей. А в целом сталкивался, например, с ПГ-баунсером, с балансировщиками, подключением их к базе, вот каких-то аспектов работы с ними?\nВопрос: А когда, условно, только был джуном? То такие задачи у нас были очень часто из-за нагрузки. Но опять, там у нас решали их задачи уже сеньоры. До меня прям как-то балансировщик не доходил. Я в курсе, что с помощью него можно распределять нагрузку, междуусловно. Серверами, и понимать, что в какой момент там, условно, отвалилось. Но на практике взаимодействия такого не было. Окей, у меня вроде всё, Борис, есть что добавить у тебя? А конкретно рассказать прям, что из себя представляет, или как? Ну, если хочешь, можно так, поверхностно. Да, ну, по сути, был опыт с Revit-ом, и задача у меня была, я помню точно, настройка эксченджев, прикручивания к очередям. Это когда только я вникал в Revit. Вот, был немножко… Смотрел на практике, как разворачивается аутбокс-паттерн. Это прямо в продакшене у нас было. Пробовал свои примеры посмотреть, тоже, как он реализовывается на своей практике. Что, по сути, в рамках Рэббита у нас есть экчейндж, есть очередь. Exchange связывается с очередью по routing.k. Если рассматривать, допустим, Kafka, то там схема работы немножко другая. Есть продюсер, есть, по сути, сама очередь. В рамках очереди есть топики, в рамках топиков могут быть партишены. И, по сути, из очереди сразу сообщение доставляется в продюсер. Вот. Как-то так, по сути. Про Outbox транзакшн тоже слышал, что он очень часто встречается для гарантии доставки сообщений. Потом, про гарантию доставки сообщений — это «at most once», то есть хотя бы один раз, как максимум, как минимум один раз,. At least once. И ещё какой там? Точно один раз. Вот я помню, что он очень такой… Его сложно реализовать на практике. Exactly once, да-да-да. Вот, что его на практике, ну, очень прям сложно реализовать, и практически невозможно. Ну, вот такие темы слышал про них. А у вас, ну, то есть…", "output": {"hard_skills": ["Entity Framework (EF)", "Управление пулом подключений", "PGBouncer", "Балансировщики нагрузки", "Revit", "Настройка Exchange и очередей", "RabbitMQ", "Outbox Pattern", "Kafka", "Гарантия доставки сообщений (at least once, exactly once)"], "soft_skills": ["Умение решать технические задачи", "Базовые знания архитектурных паттернов", "Способность к самообучению", "Коммуникабельность", "Работа в команде"], "recommendations": ["Углубить знания и практический опыт работы с балансировщиками нагрузки и их интеграцией с базами данных.", "Развивать навыки структурированного изложения мыслей для более четкого и понятного объяснения технических концепций.", "Повысить компетенции в области обеспечения гарантии доставки сообщений, особенно паттерна Exactly Once.", "Практиковаться в использовании и настройке различных инструментов для управления очередями и обменами сообщениями.", "Развивать навыки документирования процессов и решений для улучшения взаимодействия с командой и стейкхолдерами."]}, "chunk_id": 10, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Как настроена, какая гарантия была поддержана брокером вашим?\nВопрос: А тут, в зависимости от использования, допустим, у нас очень… Ну, у нас была своя система логов, то есть, вообще, самописный фреймворк. Нужно было собирать аналитику по всяким ивентам, по всяким подключенным сессиям. Здесь, когда у нас использовались логи, то при… Этой гарантии, ну, при этом функционале у нас была гарантия как максимум один раз. Во, то есть, ну, нам неважно было, потеряется сообщение в моменте или нет. Когда это уже были какие-то сложные, допустим, ну, в рамках микросервисов операции, то там уже использовался подход как минимум один раз, потому что потерять какое-то сообщение было очень, ну, так сказать, трудно. Плохо, вот, как-то так.\nОтвет: Окей, а вот если как минимум один раз гарантия доставки, вот как защититься от дублей сообщений на принимающей стороне? Вот если у тебя есть какой-то хендлер, вот как бы ты защищался? От дублей.\nВопрос: Да, в этой как раз проблеме как минимум один раз у нас может быть дубли. Там вообще, я смотрел, четыре ситуации могут быть. Ну вот, одна из них — это дубли. Допустим... Ну самый такой банальный механизм — это по какому-нибудь айдишнику в сообщении его прокидывать, и чтобы была Между ними разница. Там сообщение и конкретно идишник, допустим, пользователя уникальный. Вот. Либо если у нас event-сорсинг-архитектура по, допустим, ивенту. То есть, по ивенту уникальному мы могли в сообщение его прокидывать и его так идентифицировать, по сути, наше сообщение. Я бы…", "output": {"hard_skills": ["функциональные требования", "описание нефункциональных требований", "SLA", "нагрузочные параметры", "отказоустойчивость", "гарантии доставки сообщений (как минимум один раз, как максимум один раз)", "обработка дублирующихся сообщений", "Использование идентификаторов (ID) в сообщениях", "архитектура event-sourcing", "микросервисная архитектура", "системы логирования и анализа событий"], "soft_skills": ["умение работать в команде", "коммуникабельность", "навыки согласования требований с разработчиками и стейкхолдерами", "решение технических проблем", "аналитическое мышление"], "recommendations": ["Углубить знания в области архитектуры микросервисов и связанных с ней паттернов, таких как event-sourcing.", "Развивать навыки работы с системами логирования и анализа событий для эффективного сбора и обработки данных.", "Освоить более продвинутые техники обеспечения отказоустойчивости и управления нагрузкой в системах.", "Повышать квалификацию в области гарантии доставки сообщений и предотвращения дублей, используя различные подходы и инструменты.", "Развивать навыки документирования, чтобы более четко и ясно описывать как функциональные, так и нефункциональные требования.", "Улучшать навыки коммуникации и согласования требований с различными командами и стейкхолдерами для более эффективного сотрудничества.", "Практиковаться в аналитическом мышлении для более глубокого понимания технических проблем и разработки эффективных решений."]}, "chunk_id": 11, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: В Хендлере что происходит? То есть приходит сообщение, и дальше там мы берём айдишникова?\nВопрос: Да, ну, допустим, мы берём айдишник и, условно, его вместе с общением прокидываем. Там, допустим, структура была бы у нас какая-то. Вот. Ну, и по вот этому айдишнику мы бы различали запись. А есть ли уже такая? То есть, была бы уже она обработана или нет? Если нет, то мы её оттолкаем дальше. Если да, то в целом ничего с ней не делаем. Допустим, да, если используя, ну, если в рамках аутбокса, допустим, могу сказать, если мы вот его применяем, там для столбца можем навесить статус, либо in processing, допустим, done, либо там cancelled. Если done, то, по сути, это сообщение с этим id-шником, оно уже было обработано.\nОтвет: Вот. Ну то есть, словно исключаем Outbox, просто вот делаем такую табличку и храним там айдишники. Вот так, имя ж, веду.\nВопрос: Ну если исключаем Outbox, то в какой-нибудь табличке, да, но тут тоже должна быть гарантия, что в процессе при записи в таблицу у нас не крашнется там сервер. Ну в рамках транзакции, условно, если это делать, то я думаю проблем сильных не должно быть.", "output": {"hard_skills": ["паттерн Outbox", "управление транзакциями", "работа с базами данных", "обработка сообщений", "работа с индексами и статусами"], "soft_skills": ["аналитическое мышление", "способность к решению проблем", "внимание к деталям"], "recommendations": ["Углубить знания в паттерне Outbox и его альтернативных решениях для обеспечения надежности систем.", "Развивать навыки оптимизации работы с базами данных для повышения эффективности обработки сообщений.", "Практиковаться в более четком и структурированном изложении технических решений для улучшения коммуникации с командой."]}, "chunk_id": 12, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Окей, да, похоже на правду. А вот если чуть усложнить сценарий, например, у тебя какой-то хендлер принимает Сообщение с обновлением, да, апдейтом каким-то. И вот тебе прилетело два разных сообщения с двумя разными апдейтами, там, одного и того же поля, например. Но они по пути в брокере, там, перепутались. И тебе первым пришло Новое, а потом пришло устаревшее, и как бы оно перетрёт актуальное значение, то есть, данных. То есть, а стейт нарушится. Вот. Как от этого можно защититься? Так, структура. То есть, у нас два сообщения, и последнее перезатирает первое.\nВопрос: Сейчас надо подумать, как можно. Ну, мне почему-то на голову приходит, допустим, добавить какое-то свойство по времени, по дате создания, к примеру. Либо по дате обновления. Нет, по дате создания или обновления приходит в голову. Только надо подумать, как их проверять. Ну, скорее всего, по времени. То есть, если у нас последнее обновление было, допустим, Блин, вопрос. Ещё раз, можно вопрос? Я вот… Есть два сообщения. Они, по сути, первая обновляет, и вторая перезатирает значение. А у нас вообще такая струк… Ну, должна быть такая логика, что вот перезатирать? Ну, допустим, вот у нас есть такая логика. Например, у нас там… Ты пришёл на работу, у тебя там\nОтвет: миллион обработчиков, миллион всяких сообщений, и вот есть проблема, что сообщения могут перепутаться по пути к тебе, и они могут что-то заэффектить этим. То есть, типа, устаревшее сообщение в конце, оно может как-то, там, нарушить стейт. И вот надо эту проблему решить.\nВопрос: Ну если оно устаревшее, то по дате создания условно как-то можно. Сперва мы берём самые первые, которые шли, обрабатываем их как-то... Ну, можно навесить какой-нибудь таймер, допустим, через каждые 30 минут мы делаем обновление нашей записи. Не знаю, ну какое-то решение. Ну вот по дате.", "output": {"hard_skills": ["описание функциональных требований", "описание нефункциональных требований", "SLA", "нагрузочные параметры", "отказоустойчивость", "обработка сообщений", "управление состоянием данных", "работа с брокерами сообщений", "тайм-менеджмент в разработке"], "soft_skills": ["умение работать в команде", "коммуникабельность", "планирование", "решение проблем", "анализ", "адаптивность"], "recommendations": ["Углубить знания в области нефункциональных требований, чтобы лучше понимать, как они влияют на качество системы.", "Развивать навыки документирования, чтобы более четко и ясно описывать требования.", "Работать над формулировкой своих мыслей для более структурированного и ясного изложения решений.", "Изучить более продвинутые методы обработки сообщений и контроля версий данных, чтобы эффективно решать проблемы с конфликтующими обновлениями.", "Улучшить навыки планирования и проектирования решений для повышения устойчивости системы к сбоям и некорректным данным.", "Развивать навыки тайм-менеджмента и организации работы для более эффективного управления сложными сценариями."]}, "chunk_id": 13, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: сейчас я хочу всех сообщение есть там какой-то свой там timestamp да когда они были созданы\nВопрос: Мы айдишники уже проверяем. А, ну, так, ну, можно как? Можно применить здесь реплицирование, допустим, новые записи мы их обрабатываем, а старые мы их, допустим, просто кладём в табличку, что они, условно, были на обновлении, и никак их не трогаем, если они никак не нужны нам. Ну, помечаем, условно, статус можно для них добавить, да? Как мы помним, что они старые? А, ну, вот в голову сейчас пришло. Допустим, у вас даты есть и статус. Статус сообщения. По статусу мы, к примеру, можем для сообщения, которое вот сейчас в моменте выполняется, повесить статус «In Processing». А когда оно выполнится, мы, допустим, этот статус выдвинем в дан. И если в дан у нас успешное выполнение, то старые сообщения мы как бы, ну, не трогаем. Зачем, если у нас уже произошла запись, апдейт? Ну да, то есть мы просто их отбрасываем, они не нужны, потому что у нас более актуальный апдейт уже применен. Ну да, я, кстати, вот читал на Хабре, допустим, старые сообщения, которые уже даже не нужны, их не удаляют, а вроде бы делают репликацию и помещают, условно, в табличку, где хранятся вот… Неактуальное сообщение. Ну чтобы вот поддерживать информацию, структуру.\nОтвет: Ну это уже, да, это уже детальки.\nВопрос: Ну если… Нет, это не всегда, наверное. Если у нас есть какая-то зависимость конкретная, то, допустим, между микроси… Ну нет, здесь не всегда. Здесь, скорее всего, должна быть гарантия согласованности данных, один из принципов ASID. Вот. И такая структура, ну, не всегда пройдет. Тут, скорее всего, именно очередь, важна очередь сообщений, в каком порядке они. Угу. Ну это да, это уже в процессе обдумываешь. Ну, у нас транзакции в основном были. То есть, в рамках транзакций. И понятно, что", "output": {"hard_skills": ["управление транзакциями", "обеспечение согласованности данных", "работа с очередями сообщений"], "soft_skills": ["умение избегать детализации", "неэффективная коммуникация"], "recommendations": ["Развивать навыки подробного объяснения технических решений и концепций.", "Практиковаться в более развернутом ответе на вопросы, чтобы демонстрировать глубокое понимание предмета.", "Улучшить коммуникативные навыки для более эффективного взаимодействия с командой и заинтересованными сторонами."]}, "chunk_id": 14, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Погнали, давай про C-sharp в целом, про.NET поговорим. Вот расскажи. С какими версиями работал.NET-а? Следишь ли за обновлениями? Может, какие-то блоги читаешь, подкасты, чатики профессиональные?\nВопрос: Да, у меня опыт был с.NET Framework 4.8. Честно, не понравилось. Вот, там было прям сложно. А потом был опыт с.NET 5 и по.NET 9. По сути, все новомодные фичи, которые выходят Ну, понятно, что они не все используются, но про них стараюсь читать, как только происходит, там, апдейт в сентябре, в ноябре. Вот, стараюсь читать. К примеру, классные штуки, которые у нас очень зашли, это рекорды в.NET 5-ом появились. Потом я слышал, прикольная штука — дом, ну, дерево выражений появилось у нас в.NET-е. Frozen Dictionary, я помню, у нас архитектор про это рассказывал. Как под капотом, честно, ну, не расскажу прям детально, но смотрел. Что ещё прикольного? Ну, там, сериализация, то есть, вот, System Tech JSON, сериализация, децерилизация улучшалась по времени. А вот, кого смотрю прям тоже, ну, Анг… Джаванович, смотрю его, у него очень классный, вроде так правильно произв… его имя, классный Классный контент из русскоязычных Ну, «Программысли» бывает тоже, чекаю. Ну, у него в последнее время такие видосы. Потом «Экскалип» смотрю, там. Его собеседования какие-то тоже прикольные. Вот. Ну, и сам, плюс, в рамках компании тоже старался делать обучающий контент.\nОтвет: А ты вот сказал, тут Net9, он ещё, получается, не вышел. Ты вот превьюшку какую-то использовал? Или ты имел в виду тут Net8?\nВопрос: По.NET 9 я посмотрел, там приколы вышли с джейсонами, какие-то отступы появились, которые в работе могут пригодиться. Я просто посмотрел там.NET.NEXT.RU, что ли, вроде бы, что-то, подкасты. Вот посмотрел, послушал их. Я думал, вы уже использовали, там, тут на 9. Не-не, ну то есть у нас как бы… А когда апдейт случается, мы не сразу на него переходим, потому что могут быть разные последствия. Через год, условно, или полгода, там, когда стабильная выйдет версия. А вы на LTS-версиях сидите, или, там, 7-й, 9-й, может? Ну, у нас, к примеру, проекты были даже на.NET 6-ом, то есть мы как бы… Ну, 6-ой LTS всё-таки. Да-да-да, мы как бы не сразу делали апдейты, потому что, ну, не всегда это может сыграть роль. Вот я помню, не знаю, честно, за Дотнет 6-й вроде бы, но... Не все проекты мы на него переводили. Могу точно… Ну, могу спутать версию, «Дотент 5-й» или «Дотент 6-й», но какая-то версия была Бак, ну, проблемный, и мы не все проекты переводили сразу.", "raw_response": "{\n  \"output\": {\n    \"hard_skills\": [\n      \".NET Framework 4.8\",\n      \".NET 5\",\n      \".NET 6\",\n      \".NET 9\",\n      \"C#\",\n      \"System.Text.Json\",\n      \"резервирование словарей (Frozen Dictionary)\",\n      \"дерево выражений\",\n      \"сериализация и десериализация\"\n    ],\n    \"soft_skills\": [\n      \"самообучение и саморазвитие\",\n      \"следование за обновлениями технологий\",\n      \"умение применять новые фичи в", "chunk_id": 15, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Батчами обновление, батчами удаление. Что давно просили?\nВопрос: Ага, оно… Вот execute, delete — вот это вот. Да, у нас бачи… У нас была вроде бы самописная даже технология. Да, вроде даже самописная. Мы выполняли, если большой скоуп обновлений, у нас использовался bulk update метод, вот. Под капотом, честно, не могу сказать, как он там работал, не особо вдавался. За AFCore смотрел, там прикольный агрегейт, что-то по агрегации методы ускорились, добавились даже какие-то в новой версии. Вот. А у вас самописный был bulk update или от EF Extensions, который… классная шучечка. Вот именно что он был самописным, как я вспоминаю. Там прям был самописный отдельно либо побольше. У нас очень много было самописного по NFT Framework Core. Там даже были Expression 3. На практике я посмотрел, то есть, поковырялся немного, понял, что, ну, это довольно непростая тема на самом деле, вот. Просто пару примеров с метанита посмотрел, а в работе, так сказать, мы их не часто прям писали. Они были, но не прям must-have-практика была у нас. Довольно-таки хитрая штука. Согласен. Да, ну она… По оптимизации, можно, наверное, выиграть. Я не спорю, если её добавили. Но она очень, там… Бывали такие методы, что просто посмотришь и застрелишься, условно.\nОтвет: Слушай, вот ты наверняка, там, на SpendCore писал часто какие-то интеграции, больше ничего не делал. Вот, там, в SpendCore есть такая школа, как Http Client Factory. Вот, может, знаешь, зачем её добавили, какие она проблемы решает?\nВопрос: Пользуйтесь, если вы вообще… Да, знаю несколько проблем. Одна из них — это, ну, тоже критичная. Допустим, когда мы создаём новый HTTP-клайент, у нас может быть закончится свободный пул подключений. При HTTP-клайент, вроде там в факторе даже прописан, он как бы за нас… Занас под капотом занимается созданием и, там, условно, удалением, как фредпул, условно, вот этих http-клайентов. Это первая проблема. Вторая проблема, я слышал, связана с ДНС-ами, что Честно, сейчас прям детально не расскажу, потому что давно сталкивался, ну, читал про это, но знаю, что может быть проблема с ДНС-ами, когда у нас закончатся полы подключений, и может случиться такая проблема, что собьется ДНС. Вот.", "output": {"hard_skills": ["Batch обновление", "Batch удаление", "Bulk Update метод", "Entity Framework Core (EF Core)", "HTTP Client Factory", "Управление пулом подключений", "Оптимизация производительности", "Работа с DNS"], "soft_skills": ["Аналитическое мышление", "Коммуникабельность", "Способность к самообучению", "Умение решать технические проблемы"], "recommendations": ["Углубить практическое применение методов bulk update и batch операций для повышения эффективности работы с базами данных.", "Изучить более подробно внутреннюю реализацию HTTP Client Factory и связанные с ней проблемы, такие как управление пулом подключений и DNS.", "Развивать навыки документирования технических решений для улучшения коммуникации с командой разработки и стейкхолдерами.", "Практиковаться в объяснении сложных технических концепций более ясно и структурированно, чтобы повысить эффективность общения на технические темы."]}, "chunk_id": 16, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Да, то есть он, скорее, там, не успевает следить за обновлением DNS, если какой-то глобальный Ruby-клиент задан, вот. И когда пул у нас тут к этому толерантнее. То есть у тебя где-то далеко IP-шник поменялся, а он думает, что это ещё старый IP-шник, и, там, ошибки сыпет. Ну да, всё правильно. Окей. А про строки немного поговорим. Вот тут на эти строки они мутабельны. Какие у тебя там, видишь, плюсы и минусы?\nВопрос: Ну, минусы в том, что, условно, допустим, в форриче изменять строку — это прям выстрел в ногу, что, допустим, каждое изменение строки приводит к тому, что у нас создается новая строка. А какие минусы… Ой, ну вот это минусы, да. То есть, по производительности, плюсы. Плюсы строк. Ну, не знаю, можно ли назвать, что, допустим, если у нас есть одна строка с одинаковым названием, то она ссылается на один участок памяти. Насколько это применимо? Ну, вот, не знаю, могу предположить, что это плюс. Плюс строки. Ну, я знаю, что строки — это ссылочный тип данных, но их поведение, оно как у значимого типа данных. То есть, если мы её передаём в метод, то у нас получается копирование строки. Но ты её поменять не можешь. Соответственно, у тебя значение, оно всегда будет постоянное, в итоге. Вот. Я больше, да, имел в виду про то, что её извне, как бы, изменения мы не затронем. Вот. Что ещё про строки? Ну, про стрингбилдер. Слышал, что это классная штука, когда, условно, нужно делать много, там, контентации со строками. Про… Стрингпул слышал тоже. Ну это вот как раз, что у нас, если… Да, интернирование, что если у нас есть, допустим, три строки, и у них одинаковые значения, то они будут ссылаться на единый участок памяти. То есть всегда, имеешь в виду, любые три одинаковые строки? Ну вот, я на практике пробовал с этим поиграться. Допустим, создал перемену x, y. Если у них значение, там, три однёрки, то у них getHashCode, он будет равный. А вот в памяти они на одну область будут ссылаться или нет? А, я разбирался с этим. Ну то есть, у нас при первом создании строки… Хотя, хороший вопрос. Тут, скорее всего, зачем нам и интернирование, стринг-пул? Ну вот да, то есть. Да, да. Ну, как я смотрел, то у нас просто будет ссылание на одну ячейку в памяти. Может быть, это сделано с помощью интернирования строк? Под капотом как-то реализовано хитро.\nОтвет: могу предположить так. Ну, вот эту проблему, скорее, интернирование решает. То есть, она специальная. Если ты будешь интернировать какие-то строки, то да, будет так работать. А знаешь, какие вот по дефолту строки интернируются? Окей, а ты, получается, вот стринг-билдер, да, ты упомянул, вот как, думаю, что будет лучше, если у тебя какой-нибудь метод принимает, там, три короткие строки, там, по три символа, и тебе нужно их, там, сложить, условно, в выдаче, вот что лучше? Конкатенации встроенной, да, их сложить, там, через плюсик или… Через интерполяцию. Или, там, объявить стринг-билдер и через него складывать их и возвращать стримы.\nВопрос: Я смотрел где-то Читал пост касаемо вот этих тоже, ну, всяких прикольных моментов со строками. Я смотрел и помню, что если, допустим, у нас строка маленькая, и нужно сделать какие-то, ну, небольшие преобразования, то конкат в этом случае будет отрабатывать вроде бы быстрее, чем стрингбилдер, потому что стрингбилдер нам нужно, во-первых, инициализировать, под капотом он там выделяет сразу место, достаточно большое. И если у нас строки маленькие, ну то есть нужно с ними небольшие операции сделать, то конкатенация здесь, скорее всего, Будет маст… Ну. выдаст больше производительности.", "output": {"hard_skills": ["Понимание мутабельности и иммутабельности строк", "Использование StringBuilder для оптимизации работы со строками", "Конкатенация строк", "Понимание стринг-пула и интернирования строк", "Знание методов getHashCode", "Работа с DNS и управлением IP-адресами"], "soft_skills": ["Аналитическое мышление", "Коммуникабельность", "Способность обсуждать технические детали", "Умение решать проблемы в команде"], "recommendations": ["Улучшить структуру ответов для большей ясности и последовательности.", "Практиковаться в кратком и чётком объяснении технических концепций.", "Углубить знания в области оптимизации работы со строками и других типов данных.", "Развивать навыки эффективного взаимодействия с командой разработки и стейкхолдерами."]}, "chunk_id": 17, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Ну да, всё верно. А приходилось тебе вообще какие-нибудь утечки памяти, там, профайлить, анализировать, исправлять?\nВопрос: Какие-нибудь тузами, может, пользовался. Может, ещё бенчмарк.нетом пользовался. Вот, кстати, да-да, бенчмарк.нет я пользовался, но использовал его в рамках F-Core, в рамках коллекций, когда сам по приколу смотрел, там, условно, работу... А, там, Dictionary, и мутабельный… и мутабельного Dictionary. Ну, и мутабельный коллекция, точнее. И Frozen. Вот. Памяти. Со спаном, с пеном немножко поработал, но если вот сейчас разбирать и вспоминать, как он под капотом работает, то я не вспомню вот этого. Ну, Benchmark.net использовал. В контексте строк, да, было? Да, там, да. Я помню, мы сперва… Как сперва у нас вообще был лекция по спанами? Больше с асинхронностью, когда в стейт-машину влезал, ну, под капотом смотрел, во что разворачивается. По памяти не было такого. Окей. А ты вот про «Продикшен», Эрис, заговорил. У вас какая там… Сложность алгоритмическая поиска, например, элементов. А там такая двоякая ситуация, что если у нас, допустим, нет коллизий, то это O1. Если у нас есть коллизия, то у нас получается связанный список, и там… ОАТН, потому что нужно перебрать все элементы, которые есть в рамках линкед-листа, ну, связанного списка.\nОтвет: А Frozen дикшенари вы уже юзали, да?\nВопрос: Нет, нет, ну у нас таких примеров не было прям, не сразу как бы новые фичи юзаем. Я просто их для себя смотрю на практике, сам изучаю, потому что интересно, и вдруг, если пригодится, я сразу знаю.", "output": {"hard_skills": ["Профилирование и анализ утечек памяти", "Использование Benchmark.net", "Работа с коллекциями .NET (Dictionary, мутабельные коллекции, Frozen Dictionary)", "Использование Span", "Анализ алгоритмической сложности", "Асинхронное программирование и стейт-машины"], "soft_skills": ["Самостоятельное обучение и саморазвитие", "Интерес к новым технологиям и функциям", "Способность к решению проблем"], "recommendations": ["Углубить практические навыки работы с новыми функциями, такими как Frozen Dictionary, посредством реализации проектов или участия в открытых исходниках.", "Развивать способность запоминать и понимать внутреннюю работу используемых инструментов и библиотек для повышения технической компетентности.", "Практиковаться в более глубоких анализах и тестировании кода, чтобы улучшить навыки выявления и исправления сложных проблем."]}, "chunk_id": 18, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Да, Frozen — это прикольная тема. То есть он анализирует тип ключа и в зависимости от этого подбирает, там, нужный алгоритм. Да, я видел, там, под капотом прям тоже, ну, дерево, по сути, используется.\nВопрос: Ну, Frozen Dictionary я смотрел. Это прикольная штука, когда, условно, нам нужно загрузить элементы в словарь и больше его никогда не менять. Он в моменте выделяет очень много памяти, а локация происходит, но потом он выигрывает по производительности, как я и смотрел.\nОтвет: У меня ещё вот такой вопрос. Пользовался ли ты System Channel с каналами?\nВопрос: Систем Ченнелс? Ченнел. Я вспоминаю. Ченнел.", "output": {"hard_skills": ["знание Frozen Dictionary", "оптимизация производительности", "алгоритмы в зависимости от типа ключа", "структуры данных (деревья)"], "soft_skills": ["техническая коммуникация", "способность анализировать вопросы"], "recommendations": ["Углубить знания в области System Channels для более точного понимания и обсуждения.", "Развивать навыки четкого и понятного объяснения технических концепций во время интервью.", "Практиковать ответы на вопросы, связанные с различными структурами данных и их оптимизацией, чтобы повысить уверенность."]}, "chunk_id": 19, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Но это вроде бы тема с вип-сокетами связана, правильно? Ну нет, это такие внутренние очереди в дот-нете легковязные. Просто интересный такой в топе. Не, не восьмой, а раньше намного. То есть в шестой они точно были? Может, они в пятом появились? Или в шестом? Что-то такое. Вот, сейчас нашёл, да. Посмотрю, спасибо. Я не слышал про неё.\nВопрос: Раз мы пошли. «I observable»? А, «I observable»! Я помню, мне на собеседовании одном сказали реализовать ивент, который срабатывает при изменении коллекции. И тогда я посмотрел на «Observable Collection», если это про это.\nОтвет: На, я вот когда на Нагуляре писал, там это всё обмазано просто этими штуками. Да, и по дотам тоже. Вот это, знаете, это отдельно либо оно, ну, по производительности немного такое себе не очень, поэтому я не пользовался особо. То есть это подход прям, то есть если ты начинаешь ими пользоваться, то там всё обмазываешь как будто такой.\nВопрос: Какие, знаешь? Расскажи, что такое. Да, примитивы синхронизации делятся на два типа, как я знаю, пользовательские, на уровне ядра операционной системы, и гибридные. Там прикол в том, что вот пользовательские — это, допустим, Mutex. Монитор, что ещё у нас, семафор, они как бы идут напрямую в ядро операционной системы. А гибридные — это Slim-версии. Вот я как вспоминаю, там, семафор Slim, к примеру, что ещё у нас есть из Slim-версии. ReaderWrite, LogSlim вроде бы, что ли. Они как бы крутятся в специальном отдельном спинвейте и нужны для оптимизации, когда очень быстро выполняются у нас Блокировки и вот эти моменты. Вот. На практике использовал Mutex. Обычный лог у нас тоже был. По сути, это монитор. Что ещё? Симафор и Симафор Слим. Вот. Это именно вот в рамках работы. А сам на практике в своих подпроектах писал ещё интерлокт вроде бы на атомарной операции. Пробовал с ним играть. Вот. Как-то так. Новеньких, вот. Там я копался прям. Ну, сказать было интересно. Не сказать, но прикольно, что вот его посмотрел. Угу. Так.", "output": {"hard_skills": ["Observable Collection", "Mutex", "Semaphore", "Interlocked операции", "Примитивы синхронизации", "Slim-семафоры", "Монитор", "Interop с .NET"], "soft_skills": ["Любопытство и желание изучать новое", "Умение участвовать в обсуждениях"], "recommendations": ["Работать над структурированием ответов для более ясного и последовательного изложения мыслей.", "Улучшить навыки коммуникации, чтобы избегать неясностей и повысить понимание со стороны интервьюеров.", "Углубить понимание тонкостей использования примитивов синхронизации и их влияния на производительность.", "Практиковаться в четком определении и объяснении технических терминов и концепций.", "Развивать навыки подготовки к интервью, чтобы лучше отвечать на вопросы и избегать отклонений от темы."]}, "chunk_id": 20, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: САД, вот эти вот всё. Да, с докера начнём.\nВопрос: Сред. Сперва у нас всё в ИСе вообще разворачивалось, но потом пришли к использованию докер, создавал докер-контейнеры, по сути, с докер-файлом работал. Не сказать, что прям все детальные команды условно знаю, в процессе разбираюсь. Ну и плюс, когда свои, там, пэт-проекты пишу, Так или иначе, тоже использую докер. Очень удобно разворачивать всё.\nОтвет: Угу, а вот мультистейдж билды, например, знаешь, что такое? Мультистейдж билды? А как вот, как докер-файл был устроен, словно, вот, для дотнет-проекта?\nВопрос: А, ну, просто, по сути, стандартные команды, там, run, в зависимости от того, нужно ли использовать копирование некоторых, допустим, файлов, там, upsetting.json, нужно ли было копировать, делать mount, как я знаю, из операции. Базовый образ изначально какой был?", "output": {"hard_skills": ["Docker", "Dockerfile", "Контейнеризация", "Создание докер-контейнеров", "Работа с pet-проектами", ".NET"], "soft_skills": ["Самообучение", "Проактивность", "Умение разворачивать проекты", "Адаптивность"], "recommendations": ["Углубить знания в Docker, особенно в области мультистейдж билдов и оптимизации Dockerfile для различных технологий, таких как .NET.", "Изучить детальные команды Docker и лучшие практики для повышения эффективности использования контейнеров.", "Практиковаться в создании и оптимизации Dockerfile для сложных проектов, чтобы повысить уверенность и компетентность в инструментальных средствах контейнеризации.", "Расширить знания о базовых образах и их применении для различных типов проектов.", "Участвовать в командных проектах для улучшения навыков согласования и взаимодействия с разработчиками и стейкхолдерами."]}, "chunk_id": 21, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Базовый образ. Вот, дотнайте из SDK и рантайма, например.\nВопрос: Базовый образ, ну вроде… Блин, читаю туплю, ну вроде бы рантайм, что ли, шёл. Там, на самом деле, сбилдить можно в СДК только, получается, проект. А запускать и там, и там, например, можно. Ну, я запускал только через, именно вот, Докер, через там, PowerShell запускаю, команду выполняю, и всё. Поэтому здесь могу тупить немного.\nОтвет: но в докер файле как раз таки есть мультистейдж то есть ты сначала from sdk делаешь базовый образ sdk гильдишь в нем проект но он много весит то есть а если ты в нем и запускать будет что у тебя докер образ будет в итоге огромный там большой вот поэтому Второй стейдж используют, то есть берут from-рунтайм, и вот эти артефакты из SDK перекидывают в легковесный рунтаймовый образ, и уже там запускают прилагу, чтобы она мало весила, без там лишнего всякого. А про кэширование слоев в докере знаешь что-нибудь? Как вот оптимизировать можно? Ну вот мы сейчас поговорили, как, например, можно оптимизировать, там, размер, да, более раковестный, там, использовать базовые образы, например, вот как скорость сборки.\nВопрос: Скорой сборки, кэширование. То есть у тебя не было особо таких кейсов, чисто локального, да, юзов? Ну окей. Ну нет, ну просто как там, по работе нужно… У нас была документация, условно, в зависимости её выполняешь команды, если что-то не получается, уже разбираешься. Вот прям мультистейдж и кэширование не сталкивался.", "output": {"hard_skills": ["Docker", "многоступенчатые сборки", "SDK", "Runtime", "PowerShell"], "soft_skills": ["способность следовать документации", "адаптивность"], "recommendations": ["Углубить знания в области Docker, особенно в контексте многоступенчатых сборок и оптимизации кэша.", "Практиковать объяснение технических концепций для улучшения навыков коммуникации.", "Работать над уверенностью при ответах на вопросы, чтобы избегать неуверенности и замешательства."]}, "chunk_id": 22, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: В OPCI-CD, получается, у вас это все настроено строена на было кем-то то есть до 2 в санте условно туда\nВопрос: Не лазил, не приходилось там что менять. Ну, сам настройку не выполнял, потому что всё работало из-под коробки. Условно, залил свою пачку изменений, нажал старт и вливается там на прот или стейдж, условно. Там пайплайны выполняются. Можно к ним прикрутить разные правила. Там, допустим, в зависимости от названия ветки мы ставим разрешение, там, мердж или нет. А с Github'ом… У нас Github тоже использовался, но Не смотрел пайплайны, вот как они там выполняются, только в рамках гитхаба немного посмотрел.\nОтвет: Аркестратор у вас какой был? То есть вы куда деплоились?\nВопрос: Аркистрактор. Да, там у нас он использовался на… Ну, тоже в рамках локальной какие-то. Создать сессии быстренько для тестирования, вот. Ну, у нас всё есть в подкоробке, я прям в кибернетизм вообще не лазил. Это тоже, я как слышал, тяжёлая… Вот вам, получается, вот этот файлик диплоя-то тоже кто-то описывал?", "output": {"hard_skills": ["Настройка CI/CD пайплайнов", "Работа с GitHub", "Описание требований к деплою"], "soft_skills": ["Эффективное взаимодействие с командой", "Адаптивность к существующим процессам"], "recommendations": ["Углубить знания в области настройки и управления CI/CD пайплайнами для повышения эффективности деплоя.", "Изучить работу с оркестраторами контейнеров, такими как Kubernetes, для расширения возможностей управления инфраструктурой.", "Развивать навыки создания и оптимизации файлов деплоя для различных сред, чтобы повысить автономность в проекте.", "Освоить более глубокое использование возможностей GitHub, включая настройку и управление пайплайнами, для повышения автоматизации процессов."]}, "chunk_id": 23, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: дебопс какой-нибудь или как? Вот именно файл релиза в Кубер.\nВопрос: У нас такового прям отдельной роли для DevOps-а не было. У нас был архитектор, он, по сути, был и мамой, и папой, ну, там, закрывал вот эти бизнес-задачи. Угу. То есть, условно,\nОтвет: Не приходилось, да, менять какую-то конфигурацию, диплой, вот это вот всё? Нет, я прям… А вот со стороны кода, вот со стороны приложения, ну, были бы какие-то требования, вот какие-нибудь health-чеки, например, написать, там, какие-то обязательные пробы, вот, для кубернетиса?\nВопрос: А, конкретно для кубернетиса я не писал тоже такого. Хеллчеки я писал и работал даже с библиотекой, там, Hankfire для UI-ки, просмотр. Джоб — это вообще для Джоб вроде связано, да, Hankfire с Джобом связано. Ну, хеллчеки я писал.", "output": {"hard_skills": ["DevOps практики", "Kubernetes", "Конфигурационное управление", "Реализация health-чеки", "Работа с библиотекой Hystrix"], "soft_skills": ["Адаптивность", "Коммуникабельность", "Способность работать в условиях отсутствия специализированной роли"], "recommendations": ["Углубить знания и опыт в области DevOps, особенно в процессе деплоя и управления конфигурацией.", "Изучить более детально возможности Kubernetes и связанные с ним инструменты для управления релизами.", "Освоить дополнительные инструменты мониторинга и управления, такие как Prometheus или Grafana, чтобы расширить навыки по мониторингу приложений.", "Развивать навыки автоматизации процессов релиза для повышения эффективности работы команды."]}, "chunk_id": 24, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: Для Аки.\nВопрос: У нас был health check самый простой для бдшки. Просто, условно, делаем select, если база выполняется, значит, она жива. Потом health check у нас был для одного из микросервисов важных, который отвечал за, вот, как раз, проект C2C, то есть это просмотр Работы нашей биржи. Там прям полностью, ну, там UI-ки не было красивые, там просто огромный текст был, и по нему мы смотрели в самых начальных приложениях, типа, работает или нет.\nОтвет: Вот. Что ещё? Вот эти хеллчеки, они на что влияли?\nВопрос: По итогу, вот. Ну, с помощью хелс-чека можно было посмотреть, там, активен или… Ну, работает ли в данный момент наш, условно, наше ПО на сервере, крутится ли оно там. У нас просто за этим, типа, следили саппорты, можно их так назвать, типа. И они звонили. А так, я могу предположить, тут, скорее всего, вопрос идёт к метрикам — Графан или Приметеус. Да, во, то есть, у нас Графана была, с ней я поработал. Но в основном нам просто звонили, то есть, если что-то ночью падает, звонили, вот так. Ну это алерты как раз, я не знаю, можно ли это назвать. Уведомления у нас приходили тоже. А вот вспомнил, когда что-то ломалось, приходили просто в слаке от бота. Но ночью, к примеру, фиг посмотришь, звонки просто тупо шли. Ну да, понятно. Ну, собственно. А метрики, вот, какие-то вы отслеживали, вот, в приложении? Counter, я помню, точно использовался у нас. Метрики. Ну, у нас очень большая была метрика по нашим ивентам аналитическим, там, с какой скоростью, условно, выполняются запросы. Вот больше для этого", "output": {"hard_skills": ["Настройка и мониторинг health checks", "Работа с базами данных", "Микросервисная архитектура", "Использование Grafana и Prometheus", "Настройка алертов и уведомлений", "Метрики и аналитика"], "soft_skills": ["Коммуникабельность", "Умение работать с саппортом и командами", "Управление инцидентами"], "recommendations": ["Углубить знания в автоматизации алертов и уведомлений для повышения эффективности реагирования на инциденты.", "Развивать навыки анализа метрик для оптимизации производительности системы.", "Изучить более современные инструменты мониторинга и управления микросервисами."]}, "chunk_id": 25, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
{"instruction": "Представь, что ты виртуальный наставник. Определи сильные и слабые стороны кандидата по диалогу и сформулируй рекомендации.", "input": "Ответ: А что ещё метрики? Какие мы смотрели? Вот эти метрики, они как были реализованы? То есть это какая-то сторонняя либо там была\nВопрос: Они выставлялись в каком-то формате, там, OpenTelemetry, может, слышал про это. Вот OpenTelemetry как раз у нас и использовался. Я его и на практике тоже с ним повозился сам. Он же обычно идёт OpenTelemetry и плюс там, графана. Вот, условно, OpenTelemetric, он, не знаю, с Prometheus, он вроде тоже связан должен быть. Вот, это связка была.\nОтвет: Угу. А вот логи, ошибки — как это отслеживали? А, да, у нас.\nВопрос: У нас была своя собственная кастомная ПО для логов. В основном мы сперва, ну, мы сразу начали использовать неориентированную базу данных Clickhouse, потому что она как раз для сбора данных очень классная штука. Там под капотом движки, и в зависимости от движков, ну, там, в общем, движки на любой вкус, и как раз для логов они идеально подходили. и просто в нашей собственной системе Они там и показывались. Можно было статус выбрать, по определённым лейблам найти информацию, там, по session id, по дате и так далее. Они пошли чуть дальше, чем Елка. Да, а вы какой-нибудь Centric не использовали? Нет, чего-то такое вообще не видел, что у нас она использовалась. Ну, может, использовалась, может, я просто до неё не доходил. У нас проект был огромный, и бывало такое, что не всегда прям со всеми инструментами, ну, можно было поработать. А, да, у нас… Да, да. Я использовал интеграционный тест для микросервисов, а юнит-тесты на.NET и функциональные на Python. Правда, с функциональными было немножко. Вот. У нас на проекте вообще тестов, от слова, было совсем ноль, их просто не было. Вот. А, собственно, когда мы выкатывали новые какие-то фичи, то не всегда было такое, что, условно, тестировщик всё проверит, и мы сами могли всё проверить, что всё работает идеально. Поэтому очень часто встречались серьёзные ошибки на продакшене, что Ну, приводили, так сказать, к серьезным проблемам. И я, собственно, предложил идею. У нас команда на этом проекте была из пяти человек, чтобы начать писать юнит-тесты. Вот. Я занялся написанием... юнитесты для новых фич, а ребята как раз параллельно для старых, условно важных, функциональных моментов писали для них юнитесты. А знаешь, разница между юнит-тестами и компонентными тестами? — Компонентными? — Ну да. Ну, просто, возможно, ты писал не совсем юнит-тест, а компонентный тест, потому что они… А интеграционные в целом большой скоб каких-то компонентов под себя подразумевают и тестируют. Но в основном в рамках «Монолита» у нас был чисто МОК одного сервиса. С интеграционными мы уже делали использование разных, там, базы данных, там. А вот как… Ну, это, можно сказать, и был компонентный. Я думал, что это синоним, условно. Да, просто интеграционный тест — это ещё, как бы, отдельный этап. Угу, нет, у нас уже была развернута бд-шка, у нас тоже использовался, по сути, кликхаус для… Ну, туда попадали логи, все наши разные фичи, которые мы тестировали, связанные с какими-то… Большим объём данных, вот. У нас в основном использовался клик для этих ситуаций, чтобы по… Ну, из клихауса, как вот я на практике встречал, можно с помощью, как сказать, кавычек, доставать определённые данные там. Ну, семейство данных. Вот. У нас она уже была развернута, мы просто обращались к ней и получали набор данных. И всё, в целом. Как-то… А на ПГ у вас не было тестов? На ПГ? Да, у нас вот интеграционные часто были именно с Тлихаусом. На ПГ у нас были тесты, но он тоже был развернут в рамках уже отдельного сервера. Мы локально не поднимали их. Понятно. Ага. А, нет, нам нужно было на функциональных на Пайтоне тестировать какие-то… Я уже не помню, что-то честное я тестировал, это было давно, но вроде бы обращение к очереди, там, с эксченджами. Мы Рэббит и работоспособность Кавки смотрели с помощью Пайтон-тестов. В этом и проблема, что нужно было всем ребятам, как бы, поддерживать это всё потом. Вспоминаю, у нас использовалась такая либо как автофикстура, что ли. Автофикстура, да. Да, она как бы сразу нам и приводила нужный объём данных.", "output": {"hard_skills": ["OpenTelemetry", "Grafana", "Prometheus", "ClickHouse", "Интеграционное тестирование микросервисов", "Юнит-тестирование на .NET и Python", "Работа с базами данных", "Использование RabbitMQ и Kafka"], "soft_skills": ["Инициативность", "Командная работа", "Проблемное мышление", "Адаптивность", "Умение предлагать решения"], "recommendations": ["Изучить и освоить дополнительные инструменты логирования, такие как Centric, для расширения технического арсенала.", "Углубить знания в области автоматизации тестирования, чтобы повысить эффективность тестирования и качество выпускаемого кода.", "Развивать навыки документирования процессов и тестов для обеспечения прозрачности и понятности для всей команды.", "Повышать уровень знаний в области работы с различными базами данных и их оптимизации для улучшения производительности систем.", "Уделять внимание развитию коммуникативных навыков для более эффективного взаимодействия с большими командами и стейкхолдерами."]}, "chunk_id": 26, "source_file": "dialoge_interview_syst_analyst_9_final.json"}
